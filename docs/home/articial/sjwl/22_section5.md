#  BN与神经网络调优

## 学习目标

- 目标
  - 知道常用的一些神经网络超参数
  - 知道BN层的意义以及数学原理
- 应用
  - 无

### 神经网络调优

我们经常会涉及到参数的调优，也称之为超参数调优。目前我们从第二部分中讲过的超参数有

- 算法层面：

  - 学习率$$\alpha$$

  - $$\beta1,\beta2, \epsilon$$: Adam 优化算法的超参数，常设为 0.9、0.999、$$10^{-8}$$
  - $$\lambda$$：正则化网络参数，

- 网络层面：

  - hidden units：各隐藏层神经元个数
  - layers：神经网络层数

####  调参技巧

对于调参，通常采用跟机器学习中介绍的网格搜索一致，让所有参数的可能组合在一起，得到N组结果。然后去测试每一组的效果去选择。

> 假设我们现在有两个参数
>
> $$\alpha$$: 0.1,0.01,0.001，$$\beta$$:0.8,0.88,0.9
>
> 这样会有9种组合，[0.1, 0.8], [0.1, 0.88], [0.1, 0.9]…….

* 合理的参数设置
  * 学习率$$\alpha$$：0.0001、0.001、0.01、0.1，跨度稍微大一些。
  * 算法参数$$\beta$$， 0.999、0.9995、0.998等，尽可能的选择接近于1的值

注：而指数移动平均值参数：β 从 0.9 （相当于近10天的影响）增加到 0.9005 对结果（1/(1-β)）几乎没有影响，而 β 从 0.999 到 0.9995 对结果的影响会较大，因为是指数级增加。通过介绍过的式子理解$$S_{100} = 0.1Y_{100} + 0.1 * 0.9Y_{99} + 0.1 * {(0.9)}^2Y_{98} + ...$$

#### 运行

通常我们有这么多参数组合，每一个组合运行训练都需要很长时间，但是如果资源允许的话，可以同时并行的训练多个参数模型，并观察效果。如果资源不允许的话，还是得一个模型一个模型的运行，并时刻观察损失的变化

**所以对于这么多的超参数，调优是一件复杂的事情，怎么让这么多的超参数范围，工作效果还能达到更好，训练变得更容易呢？**

### 批标准化（Batch Normalization）

Batch Normalization论文地址：https://arxiv.org/abs/1502.03167

其中最开头介绍是这样的：

![](/img/articial/BN论文.png)

> * **我们将这种现象称为内部协变量偏移 ，并通过 标准化层 输入来解决问题**。
>
> * 可以消除对Dropout的需求。
>
> * 应用于最先进的图像分类模型，批量标准化实现了相同的精度，培训步骤减少了14倍，并且显着地超过了原始模型。使用批量标准化网络的集合，我们改进了ImageNet分类的最佳发布结果：达到4.9％的前5个验证错误（和4.8％的测试错误），超出了人类评估者的准确性。

首先我们还是回到之前，我们对输入特征 X 使用了标准化处理。标准化化后的优化得到了加速。

对于深层网络呢？我们接下来看一下这个公式，这是向量的表示。表示每Mini-batch有m个样本。

- m个样本的向量表示

> $$Z^{[L]} = W^{[L]}A^{[L-1]}+b^{[L]}$$
>
> $$A^{[L]}=g^{[L]}(Z^{[L]})$$
>
> 输入$$A^{[L-1]}$$

深层网络当中不止是初始的特征输入，而到了隐藏层也有输出结果，所以我们是否能够对隐层的输入$$Z^{[L]}$$进行标准化

![](/img/articial/4层网络.png)

#### 批标准化公式

* 1、所以假设对于上图第二个四个神经元隐层。记做$$Z^{[l]}$$，对于每一层的输出，进行标准化操作

$$\mu = \frac{1}{m} \sum_i z^{(i)}$$

$$\sigma^2 = \frac{1}{m} \sum_i {(z_i - \mu)}^2$$

$$z_{norm}^{(i)} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}}$$

注：其中$$\epsilon$$是为了防止分母为0，取值$$10^{-8}$$。这样使得所有的l层输入$$z^{[l]}_{[i]}$$为 0，方差为 1。

* 2、但是原文的作者不想让隐藏层单元总是含有平均值 0 和方差 1，他认为也许隐藏层单元有了不同的分布会更有意义。因此，我们会增加这样的参数

$$\tilde z^{(i)} = \gamma z^{(i)}_{norm} + \beta$$

注：其中，$$\gamma$$和$$\beta$$都是模型的学习参数（如同W和b一样），所以可以用各种梯度下降算法来更新 γ 和 β 的值，如同更新神经网络的权重一样。

* 为什么要使用这样两个参数

如果**各隐藏层的输入均值在靠近0的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。**因此，需要用 γ 和 β 对标准化后的结果做进一步处理。

#### 过程图

![](/img/articial/BN过程.png)

每一层中都会有两个参数$$\beta, \gamma$$。

注：原论文的公式图

![](/img/articial/BN标准化公式.png)

#### 为什么批标准化能够是优化过程变得简单

我们之前在原文中标记了一个问题叫做叫做"internal covariate shift"。这个词翻译叫做协变量偏移，那么有一个解释叫做 **在网络当中数据的分布会随着不同数据集改变** 。这是网络中存在的问题。那我们一起来看一下数据本身分布是在这里会有什么问题。

![](/img/articial/训练集测试集分布.png)

也就是说如果我们在训练集中的数据分布如左图，那么网络当中学习到的分布状况也就是左图。那对于给定一个测试集中的数据，分布不一样。这个网络可能就不能准确去区分。这种情况下，一般要对模型进行重新训练。

* **1、Batch Normalization的作用就是减小Internal Covariate Shift 所带来的影响，让模型变得更加健壮，鲁棒性（Robustness）更强。**
* **2 、Batch Normalization 的作用，使得均值和方差保持固定（由每一层$$\gamma$$和$$\beta$$决定），不同层学习到不同的分布状态**
* **3、因此后层的学习变得更容易一些。Batch Normalization 减少了各层 W 和 b 之间的耦合性，让各层更加独立，实现自我训练学习的效果**

#### BN总结

Batch Normalization 也起到微弱的正则化效果，但是不要将 Batch Normalization 作为正则化的手段，而是当作加速学习的方式。**Batch Normalization主要解决的还是反向传播过程中的梯度问题（梯度消失和爆炸）。**

###  总结

* 掌握基本的超参数以及调参技巧
* 掌握BN的原理以及作用
