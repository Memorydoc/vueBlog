(window.webpackJsonp=window.webpackJsonp||[]).push([[23],{296:function(s,t,a){"use strict";a.r(t);var n=a(10),r=Object(n.a)({},function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"案例：实现线性回归"}},[s._v("案例：实现线性回归")]),s._v(" "),a("h2",{attrs:{id:"学习目标"}},[s._v("学习目标")]),s._v(" "),a("ul",[a("li",[s._v("目标\n"),a("ul",[a("li",[s._v("应用op的name参数实现op的名字修改")]),s._v(" "),a("li",[s._v("应用variable_scope实现图程序作用域的添加")]),s._v(" "),a("li",[s._v("应用scalar或histogram实现张量值的跟踪显示")]),s._v(" "),a("li",[s._v("应用merge_all实现张量值的合并")]),s._v(" "),a("li",[s._v("应用add_summary实现张量值写入文件")]),s._v(" "),a("li",[s._v("应用tf.train.saver实现TensorFlow的模型保存以及加载")]),s._v(" "),a("li",[s._v("应用tf.app.flags实现命令行参数添加和使用")]),s._v(" "),a("li",[s._v("应用reduce_mean、square实现均方误差计算")]),s._v(" "),a("li",[s._v("应用tf.train.GradientDescentOptimizer实现有梯度下降优化器创建")]),s._v(" "),a("li",[s._v("应用minimize函数优化损失")]),s._v(" "),a("li",[s._v("知道梯度爆炸以及常见解决技巧")])])]),s._v(" "),a("li",[s._v("应用\n"),a("ul",[a("li",[s._v("实现线性回归模型")])])]),s._v(" "),a("li",[s._v("内容预览\n"),a("ul",[a("li",[s._v("2.7.1 线性回归原理复习")]),s._v(" "),a("li",[s._v("2.7.2 案例：实现线性回归的训练")]),s._v(" "),a("li",[s._v("2.7.3 增加其他功能\n"),a("ul",[a("li",[s._v("1 增加变量显示")]),s._v(" "),a("li",[s._v("2 增加命名空间")]),s._v(" "),a("li",[s._v("3 模型的保存与加载")]),s._v(" "),a("li",[s._v("4 命令行参数使用")])])])])])]),s._v(" "),a("h2",{attrs:{id:"线性回归原理复习"}},[s._v("线性回归原理复习")]),s._v(" "),a("p",[s._v("根据数据建立回归模型，w1x1+w2x2+…..+b = y，通过真实值与预测值之间建立误差，使用梯度下降优化得到损失最小对应的权重和偏置。最终确定模型的权重和偏置参数。最后可以用这些参数进行预测。")]),s._v(" "),a("h2",{attrs:{id:"案例：实现线性回归的训练"}},[s._v("案例：实现线性回归的训练")]),s._v(" "),a("h3",{attrs:{id:"_1-案例确定"}},[s._v("1 案例确定")]),s._v(" "),a("ul",[a("li",[s._v("假设随机指定100个点，只有一个特征")]),s._v(" "),a("li",[s._v("数据本身的分布为 y = 0.8 * x + 0.7")])]),s._v(" "),a("blockquote",[a("p",[s._v("这里将数据分布的规律确定，是为了使我们训练出的参数跟真实的参数（即0.8和0.7）比较是否训练准确")])]),s._v(" "),a("h3",{attrs:{id:"api"}},[s._v("API")]),s._v(" "),a("p",[a("strong",[s._v("运算")])]),s._v(" "),a("ul",[a("li",[s._v("矩阵运算\n"),a("ul",[a("li",[s._v("tf.matmul(x, w)")])])]),s._v(" "),a("li",[s._v("平方\n"),a("ul",[a("li",[s._v("tf.square(error)")])])]),s._v(" "),a("li",[s._v("均值\n"),a("ul",[a("li",[s._v("tf.reduce_mean(error)")])])])]),s._v(" "),a("p",[a("strong",[s._v("梯度下降优化")])]),s._v(" "),a("ul",[a("li",[s._v("tf.train.GradientDescentOptimizer(learning_rate)\n"),a("ul",[a("li",[s._v("梯度下降优化")]),s._v(" "),a("li",[s._v("learning_rate:学习率，一般为0~1之间比较小的值")]),s._v(" "),a("li",[s._v("method:\n"),a("ul",[a("li",[s._v("minimize(loss)")])])]),s._v(" "),a("li",[s._v("return:梯度下降op")])])])]),s._v(" "),a("h3",{attrs:{id:"步骤分析"}},[s._v("步骤分析")]),s._v(" "),a("ul",[a("li",[s._v("1 准备好数据集：y = 0.8x + 0.7 100个样本")]),s._v(" "),a("li",[s._v("2 建立线性模型\n"),a("ul",[a("li",[s._v("随机初始化W1和b1")]),s._v(" "),a("li",[s._v("y = W·X + b，目标：求出权重W和偏置b")])])]),s._v(" "),a("li",[s._v("3 确定损失函数（预测值与真实值之间的误差）-均方误差")]),s._v(" "),a("li",[s._v("4 梯度下降优化损失：需要指定学习率（超参数）")])]),s._v(" "),a("h3",{attrs:{id:"实现完整功能"}},[s._v("实现完整功能")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" tf\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" os\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("linear_regression")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v('"""\n    自实现线性回归\n    :return: None\n    """')]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 1）准备好数据集：y = 0.8x + 0.7 100个样本")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 特征值X, 目标值y_true")]),s._v("\n    X "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" mean"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" stddev"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# y_true [100, 1]")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 矩阵运算 X（100， 1）* （1, 1）= y_true(100, 1)")]),s._v("\n    y_true "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("matmul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.7")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2）建立线性模型：")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# y = W·X + b，目标：求出权重W和偏置b")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 3）随机初始化W1和b1")]),s._v("\n    weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("initial_value"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    bias "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("initial_value"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("matmul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" bias\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 4）确定损失函数（预测值与真实值之间的误差）-均方误差")]),s._v("\n    error "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("square"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v(" y_true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 5）梯度下降优化损失：需要指定学习率（超参数）")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# W2 = W1 - 学习率*(方向)")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# b2 = b1 - 学习率*(方向)")]),s._v("\n    optimizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("GradientDescentOptimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("learning_rate"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.01")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("minimize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 初始化变量")]),s._v("\n    init "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("global_variables_initializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 开启会话进行训练")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Session"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 运行初始化变量Op")]),s._v("\n        sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("init"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"随机初始化的权重为%f， 偏置为%f"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 训练模型")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n            sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"第%d步的误差为%f，权重为%f， 偏置为%f"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br")])]),a("h3",{attrs:{id:"变量的trainable设置观察"}},[s._v("变量的trainable设置观察")]),s._v(" "),a("p",[s._v("trainable的参数作用，指定是否训练")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" mean"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" stddev"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"weights"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" trainable"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("h2",{attrs:{id:"增加其他功能"}},[s._v("增加其他功能")]),s._v(" "),a("ul",[a("li",[a("strong",[s._v("增加命名空间")])]),s._v(" "),a("li",[a("strong",[s._v("命令行参数设置")])])]),s._v(" "),a("h3",{attrs:{id:"增加命名空间"}},[s._v("增加命名空间")]),s._v(" "),a("p",[s._v("是代码结构更加清晰，Tensorboard图结构清楚")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variable_scope"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"lr_model"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("linear_regression")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 1）准备好数据集：y = 0.8x + 0.7 100个样本")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 特征值X, 目标值y_true")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variable_scope"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"original_data"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        X "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" mean"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" stddev"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"original_data_x"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# y_true [100, 1]")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 矩阵运算 X（100， 1）* （1, 1）= y_true(100, 1)")]),s._v("\n        y_true "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("matmul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"original_matmul"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.7")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2）建立线性模型：")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# y = W·X + b，目标：求出权重W和偏置b")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 3）随机初始化W1和b1")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variable_scope"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"linear_model"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("initial_value"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"weights"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        bias "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("initial_value"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bias"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("matmul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"model_matmul"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" bias\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 4）确定损失函数（预测值与真实值之间的误差）-均方误差")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variable_scope"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"loss"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        error "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("square"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v(" y_true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"error_op"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 5）梯度下降优化损失：需要指定学习率（超参数）")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# W2 = W1 - 学习率*(方向)")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# b2 = b1 - 学习率*(方向)")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variable_scope"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"gd_optimizer"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        optimizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("GradientDescentOptimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("learning_rate"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.01")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"optimizer"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("minimize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2）收集变量")]),s._v("\n    tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("scalar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"error"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("histogram"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"weights"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("histogram"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bias"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 3）合并变量")]),s._v("\n    merge "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("merge_all"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 初始化变量")]),s._v("\n    init "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("global_variables_initializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 开启会话进行训练")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Session"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 运行初始化变量Op")]),s._v("\n        sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("init"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"随机初始化的权重为%f， 偏置为%f"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 1）创建事件文件")]),s._v("\n        file_writer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("FileWriter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("logdir"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./summary"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" graph"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 训练模型")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n            sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"第%d步的误差为%f，权重为%f， 偏置为%f"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 4）运行合并变量op")]),s._v("\n            summary "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("merge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            file_writer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br"),a("span",{staticClass:"line-number"},[s._v("44")]),a("br"),a("span",{staticClass:"line-number"},[s._v("45")]),a("br"),a("span",{staticClass:"line-number"},[s._v("46")]),a("br"),a("span",{staticClass:"line-number"},[s._v("47")]),a("br"),a("span",{staticClass:"line-number"},[s._v("48")]),a("br"),a("span",{staticClass:"line-number"},[s._v("49")]),a("br"),a("span",{staticClass:"line-number"},[s._v("50")]),a("br")])]),a("h3",{attrs:{id:"模型的保存与加载"}},[s._v("模型的保存与加载")]),s._v(" "),a("ul",[a("li",[a("strong",[s._v("tf.train.Saver(var_list=None,max_to_keep=5)")]),s._v(" "),a("ul",[a("li",[s._v("保存和加载模型（保存文件格式：checkpoint文件）")]),s._v(" "),a("li",[s._v("var_list:指定将要保存和还原的变量。它可以作为一个dict或一个列表传递.")]),s._v(" "),a("li",[s._v("max_to_keep：指示要保留的最近检查点文件的最大数量。创建新文件时，会删除较旧的文件。如果无或0，则保留所有检查点文件。默认为5（即保留最新的5个检查点文件。）")])])])]),s._v(" "),a("p",[s._v("使用")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("例如：\n指定目录"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v("模型名字\nsaver"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("save"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'/tmp/ckpt/test/myregression.ckpt'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nsaver"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("restore"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'/tmp/ckpt/test/myregression.ckpt'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("如要判断模型是否存在，直接指定目录")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("checkpoint "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("latest_checkpoint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./tmp/model/"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nsaver"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("restore"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" checkpoint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("h3",{attrs:{id:"_4-命令行参数使用"}},[s._v("4 命令行参数使用")]),s._v(" "),a("ul",[a("li",[s._v("1、"),a("img",{attrs:{src:"/img/articial/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0.png",alt:""}})])]),s._v(" "),a("ul",[a("li",[s._v("2、 tf.app.flags.,在flags有一个FLAGS标志，它在程序中可以调用到我们")])]),s._v(" "),a("p",[s._v("前面具体定义的flag_name")]),s._v(" "),a("ul",[a("li",[s._v("3、通过tf.app.run()启动main(argv)函数")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 定义一些常用的命令行参数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 训练步数")]),s._v("\ntf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("DEFINE_integer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"max_step"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"训练模型的步数"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 定义模型的路径")]),s._v("\ntf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("DEFINE_string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"model_dir"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('" "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"模型保存的路径+模型名字"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 定义获取命令行参数")]),s._v("\nFLAGS "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("FLAGS\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 开启训练")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 训练的步数（依据模型大小而定）")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("FLAGS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("max_step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n     sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("train_op"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br")])]),a("h3",{attrs:{id:"完整代码"}},[s._v("完整代码")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" tf\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" os\n\ntf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("DEFINE_string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"model_path"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./linear_regression/"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"模型保存的路径和文件名"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nFLAGS "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("FLAGS\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("linear_regression")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 1）准备好数据集：y = 0.8x + 0.7 100个样本")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 特征值X, 目标值y_true")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variable_scope"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"original_data"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        X "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" mean"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" stddev"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"original_data_x"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# y_true [100, 1]")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 矩阵运算 X（100， 1）* （1, 1）= y_true(100, 1)")]),s._v("\n        y_true "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("matmul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"original_matmul"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.7")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2）建立线性模型：")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# y = W·X + b，目标：求出权重W和偏置b")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 3）随机初始化W1和b1")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variable_scope"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"linear_model"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("initial_value"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"weights"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        bias "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("initial_value"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bias"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("matmul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"model_matmul"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" bias\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 4）确定损失函数（预测值与真实值之间的误差）-均方误差")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variable_scope"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"loss"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        error "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("square"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v(" y_true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"error_op"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 5）梯度下降优化损失：需要指定学习率（超参数）")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# W2 = W1 - 学习率*(方向)")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# b2 = b1 - 学习率*(方向)")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variable_scope"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"gd_optimizer"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        optimizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("GradientDescentOptimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("learning_rate"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.01")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"optimizer"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("minimize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2）收集变量")]),s._v("\n    tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("scalar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"error"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("histogram"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"weights"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("histogram"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"bias"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 3）合并变量")]),s._v("\n    merge "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("merge_all"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 初始化变量")]),s._v("\n    init "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("global_variables_initializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 开启会话进行训练")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Session"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 运行初始化变量Op")]),s._v("\n        sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("init"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 未经训练的权重和偏置")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"随机初始化的权重为%f， 偏置为%f"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 当存在checkpoint文件，就加载模型")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 1）创建事件文件")]),s._v("\n        file_writer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("FileWriter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("logdir"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./summary"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" graph"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 训练模型")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n            sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"第%d步的误差为%f，权重为%f， 偏置为%f"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 4）运行合并变量op")]),s._v("\n            summary "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("merge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            file_writer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n          \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),s._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("main")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("argv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"这是main函数"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("argv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("FLAGS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    linear_regression"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("==")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"__main__"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br"),a("span",{staticClass:"line-number"},[s._v("44")]),a("br"),a("span",{staticClass:"line-number"},[s._v("45")]),a("br"),a("span",{staticClass:"line-number"},[s._v("46")]),a("br"),a("span",{staticClass:"line-number"},[s._v("47")]),a("br"),a("span",{staticClass:"line-number"},[s._v("48")]),a("br"),a("span",{staticClass:"line-number"},[s._v("49")]),a("br"),a("span",{staticClass:"line-number"},[s._v("50")]),a("br"),a("span",{staticClass:"line-number"},[s._v("51")]),a("br"),a("span",{staticClass:"line-number"},[s._v("52")]),a("br"),a("span",{staticClass:"line-number"},[s._v("53")]),a("br"),a("span",{staticClass:"line-number"},[s._v("54")]),a("br"),a("span",{staticClass:"line-number"},[s._v("55")]),a("br"),a("span",{staticClass:"line-number"},[s._v("56")]),a("br"),a("span",{staticClass:"line-number"},[s._v("57")]),a("br"),a("span",{staticClass:"line-number"},[s._v("58")]),a("br"),a("span",{staticClass:"line-number"},[s._v("59")]),a("br"),a("span",{staticClass:"line-number"},[s._v("60")]),a("br"),a("span",{staticClass:"line-number"},[s._v("61")]),a("br"),a("span",{staticClass:"line-number"},[s._v("62")]),a("br"),a("span",{staticClass:"line-number"},[s._v("63")]),a("br"),a("span",{staticClass:"line-number"},[s._v("64")]),a("br"),a("span",{staticClass:"line-number"},[s._v("65")]),a("br"),a("span",{staticClass:"line-number"},[s._v("66")]),a("br"),a("span",{staticClass:"line-number"},[s._v("67")]),a("br"),a("span",{staticClass:"line-number"},[s._v("68")]),a("br"),a("span",{staticClass:"line-number"},[s._v("69")]),a("br"),a("span",{staticClass:"line-number"},[s._v("70")]),a("br"),a("span",{staticClass:"line-number"},[s._v("71")]),a("br")])]),a("h3",{attrs:{id:"作业：将面向过程改为面向对象"}},[s._v("作业：将面向过程改为面向对象")]),s._v(" "),a("p",[s._v("参考代码")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 用tensorflow自实现一个线性回归案例")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 定义一些常用的命令行参数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 训练步数")]),s._v("\ntf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("DEFINE_integer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"max_step"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"训练模型的步数"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 定义模型的路径")]),s._v("\ntf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("DEFINE_string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"model_dir"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('" "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"模型保存的路径+模型名字"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nFLAGS "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("flags"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("FLAGS\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("MyLinearRegression")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("object")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v('"""\n    自实现线性回归\n    """')]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("pass")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("inputs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v('"""\n        获取特征值目标值数据数据\n        :return:\n        """')]),s._v("\n        x_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" mean"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" stddev"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"x_data"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        y_true "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("matmul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" x_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_true\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("inference")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v('"""\n        根据输入数据建立模型\n        :param feature:\n        :param label:\n        :return:\n        """')]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variable_scope"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"linea_model"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2、建立回归模型，分析别人的数据的特征数量---\x3e权重数量， 偏置b")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 由于有梯度下降算法优化，所以一开始给随机的参数，权重和偏置")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 被优化的参数，必须得使用变量op去定义")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 变量初始化权重和偏置")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# weight 2维[1, 1]    bias [1]")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 变量op当中会有trainable参数决定是否训练")]),s._v("\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random_normal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" mean"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" stddev"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                                 name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"weights"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n            self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("bias "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'biases'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 建立回归公式去得出预测结果")]),s._v("\n            y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("matmul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("bias\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" y_predict\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("loss")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v('"""\n        目标值和真实值计算损失\n        :return: loss\n        """')]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 3、求出我们模型跟真实数据之间的损失")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 均方误差公式")]),s._v("\n        loss "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("square"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_true "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v(" y_predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" loss\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("merge_summary")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 1、收集张量的值")]),s._v("\n        tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("scalar"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"losses"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("histogram"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"w"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("histogram"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'b'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2、合并变量")]),s._v("\n        merged "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("merge_all"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" merged\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("sgd_op")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v('"""\n        获取训练OP\n        :return:\n        """')]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 4、使用梯度下降优化器优化")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 填充学习率：0 ~ 1    学习率是非常小，")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 学习率大小决定你到达损失一个步数多少")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 最小化损失")]),s._v("\n        train_op "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("GradientDescentOptimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("minimize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" train_op\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("train")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v('"""\n        训练模型\n        :param loss:\n        :return:\n        """')]),s._v("\n\n        g "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get_default_graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" g"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("as_default"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n            x_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_true "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n            y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("inference"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n            loss "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_true"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n            train_op "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sgd_op"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 收集观察的结果值")]),s._v("\n            merged "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("merge_summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n            saver "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Saver"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Session"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n                sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("global_variables_initializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n                \n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 在没训练，模型的参数值")]),s._v("\n        \t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"初始化的权重：%f, 偏置：%f"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 开启训练")]),s._v("\n                "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 训练的步数（依据模型大小而定）")]),s._v("\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("FLAGS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("max_step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n                    sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("train_op"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 生成事件文件，观察图结构")]),s._v("\n                    file_writer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("FileWriter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./tmp/summary/"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" graph"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("graph"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n                    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"训练第%d步之后的损失:%f, 权重：%f, 偏置：%f"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n                        i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                        loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("bias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("eval")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 运行收集变量的结果")]),s._v("\n                    summary "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("merged"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n                    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 添加到文件")]),s._v("\n                    file_writer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("summary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("==")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'__main__'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    lr "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" MyLinearRegression"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    lr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br"),a("span",{staticClass:"line-number"},[s._v("44")]),a("br"),a("span",{staticClass:"line-number"},[s._v("45")]),a("br"),a("span",{staticClass:"line-number"},[s._v("46")]),a("br"),a("span",{staticClass:"line-number"},[s._v("47")]),a("br"),a("span",{staticClass:"line-number"},[s._v("48")]),a("br"),a("span",{staticClass:"line-number"},[s._v("49")]),a("br"),a("span",{staticClass:"line-number"},[s._v("50")]),a("br"),a("span",{staticClass:"line-number"},[s._v("51")]),a("br"),a("span",{staticClass:"line-number"},[s._v("52")]),a("br"),a("span",{staticClass:"line-number"},[s._v("53")]),a("br"),a("span",{staticClass:"line-number"},[s._v("54")]),a("br"),a("span",{staticClass:"line-number"},[s._v("55")]),a("br"),a("span",{staticClass:"line-number"},[s._v("56")]),a("br"),a("span",{staticClass:"line-number"},[s._v("57")]),a("br"),a("span",{staticClass:"line-number"},[s._v("58")]),a("br"),a("span",{staticClass:"line-number"},[s._v("59")]),a("br"),a("span",{staticClass:"line-number"},[s._v("60")]),a("br"),a("span",{staticClass:"line-number"},[s._v("61")]),a("br"),a("span",{staticClass:"line-number"},[s._v("62")]),a("br"),a("span",{staticClass:"line-number"},[s._v("63")]),a("br"),a("span",{staticClass:"line-number"},[s._v("64")]),a("br"),a("span",{staticClass:"line-number"},[s._v("65")]),a("br"),a("span",{staticClass:"line-number"},[s._v("66")]),a("br"),a("span",{staticClass:"line-number"},[s._v("67")]),a("br"),a("span",{staticClass:"line-number"},[s._v("68")]),a("br"),a("span",{staticClass:"line-number"},[s._v("69")]),a("br"),a("span",{staticClass:"line-number"},[s._v("70")]),a("br"),a("span",{staticClass:"line-number"},[s._v("71")]),a("br"),a("span",{staticClass:"line-number"},[s._v("72")]),a("br"),a("span",{staticClass:"line-number"},[s._v("73")]),a("br"),a("span",{staticClass:"line-number"},[s._v("74")]),a("br"),a("span",{staticClass:"line-number"},[s._v("75")]),a("br"),a("span",{staticClass:"line-number"},[s._v("76")]),a("br"),a("span",{staticClass:"line-number"},[s._v("77")]),a("br"),a("span",{staticClass:"line-number"},[s._v("78")]),a("br"),a("span",{staticClass:"line-number"},[s._v("79")]),a("br"),a("span",{staticClass:"line-number"},[s._v("80")]),a("br"),a("span",{staticClass:"line-number"},[s._v("81")]),a("br"),a("span",{staticClass:"line-number"},[s._v("82")]),a("br"),a("span",{staticClass:"line-number"},[s._v("83")]),a("br"),a("span",{staticClass:"line-number"},[s._v("84")]),a("br"),a("span",{staticClass:"line-number"},[s._v("85")]),a("br"),a("span",{staticClass:"line-number"},[s._v("86")]),a("br"),a("span",{staticClass:"line-number"},[s._v("87")]),a("br"),a("span",{staticClass:"line-number"},[s._v("88")]),a("br"),a("span",{staticClass:"line-number"},[s._v("89")]),a("br"),a("span",{staticClass:"line-number"},[s._v("90")]),a("br"),a("span",{staticClass:"line-number"},[s._v("91")]),a("br"),a("span",{staticClass:"line-number"},[s._v("92")]),a("br"),a("span",{staticClass:"line-number"},[s._v("93")]),a("br"),a("span",{staticClass:"line-number"},[s._v("94")]),a("br"),a("span",{staticClass:"line-number"},[s._v("95")]),a("br"),a("span",{staticClass:"line-number"},[s._v("96")]),a("br"),a("span",{staticClass:"line-number"},[s._v("97")]),a("br"),a("span",{staticClass:"line-number"},[s._v("98")]),a("br"),a("span",{staticClass:"line-number"},[s._v("99")]),a("br"),a("span",{staticClass:"line-number"},[s._v("100")]),a("br"),a("span",{staticClass:"line-number"},[s._v("101")]),a("br"),a("span",{staticClass:"line-number"},[s._v("102")]),a("br"),a("span",{staticClass:"line-number"},[s._v("103")]),a("br"),a("span",{staticClass:"line-number"},[s._v("104")]),a("br"),a("span",{staticClass:"line-number"},[s._v("105")]),a("br"),a("span",{staticClass:"line-number"},[s._v("106")]),a("br"),a("span",{staticClass:"line-number"},[s._v("107")]),a("br"),a("span",{staticClass:"line-number"},[s._v("108")]),a("br"),a("span",{staticClass:"line-number"},[s._v("109")]),a("br"),a("span",{staticClass:"line-number"},[s._v("110")]),a("br"),a("span",{staticClass:"line-number"},[s._v("111")]),a("br"),a("span",{staticClass:"line-number"},[s._v("112")]),a("br"),a("span",{staticClass:"line-number"},[s._v("113")]),a("br"),a("span",{staticClass:"line-number"},[s._v("114")]),a("br"),a("span",{staticClass:"line-number"},[s._v("115")]),a("br"),a("span",{staticClass:"line-number"},[s._v("116")]),a("br"),a("span",{staticClass:"line-number"},[s._v("117")]),a("br"),a("span",{staticClass:"line-number"},[s._v("118")]),a("br"),a("span",{staticClass:"line-number"},[s._v("119")]),a("br"),a("span",{staticClass:"line-number"},[s._v("120")]),a("br"),a("span",{staticClass:"line-number"},[s._v("121")]),a("br"),a("span",{staticClass:"line-number"},[s._v("122")]),a("br"),a("span",{staticClass:"line-number"},[s._v("123")]),a("br"),a("span",{staticClass:"line-number"},[s._v("124")]),a("br"),a("span",{staticClass:"line-number"},[s._v("125")]),a("br"),a("span",{staticClass:"line-number"},[s._v("126")]),a("br"),a("span",{staticClass:"line-number"},[s._v("127")]),a("br"),a("span",{staticClass:"line-number"},[s._v("128")]),a("br"),a("span",{staticClass:"line-number"},[s._v("129")]),a("br"),a("span",{staticClass:"line-number"},[s._v("130")]),a("br"),a("span",{staticClass:"line-number"},[s._v("131")]),a("br"),a("span",{staticClass:"line-number"},[s._v("132")]),a("br"),a("span",{staticClass:"line-number"},[s._v("133")]),a("br"),a("span",{staticClass:"line-number"},[s._v("134")]),a("br"),a("span",{staticClass:"line-number"},[s._v("135")]),a("br"),a("span",{staticClass:"line-number"},[s._v("136")]),a("br"),a("span",{staticClass:"line-number"},[s._v("137")]),a("br"),a("span",{staticClass:"line-number"},[s._v("138")]),a("br"),a("span",{staticClass:"line-number"},[s._v("139")]),a("br"),a("span",{staticClass:"line-number"},[s._v("140")]),a("br"),a("span",{staticClass:"line-number"},[s._v("141")]),a("br"),a("span",{staticClass:"line-number"},[s._v("142")]),a("br"),a("span",{staticClass:"line-number"},[s._v("143")]),a("br"),a("span",{staticClass:"line-number"},[s._v("144")]),a("br")])])])},[],!1,null,null,null);t.default=r.exports}}]);