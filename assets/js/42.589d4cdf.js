(window.webpackJsonp=window.webpackJsonp||[]).push([[42],{315:function(s,t,a){"use strict";a.r(t);var n=a(10),e=Object(n.a)({},function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"tensorflow实现神经网络"}},[s._v("Tensorflow实现神经网络")]),s._v(" "),a("h2",{attrs:{id:"学习目标"}},[s._v("学习目标")]),s._v(" "),a("ul",[a("li",[s._v("目标\n"),a("ul",[a("li",[s._v("掌握Tensorflow 模型API的使用")])])]),s._v(" "),a("li",[s._v("应用\n"),a("ul",[a("li",[s._v("应用TF搭建一个时尚分类分类模型")])])])]),s._v(" "),a("h2",{attrs:{id:"tf-keras构建模型训练评估测试api介绍"}},[s._v("tf.keras构建模型训练评估测试API介绍")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" tf\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" keras\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("h3",{attrs:{id:"构建模型"}},[s._v("构建模型")]),s._v(" "),a("ul",[a("li",[a("strong",[s._v("1、Keras中模型类型：Sequential模型")]),s._v(" "),a("ul",[a("li",[s._v("在 Keras 中，您可以通过组合层来构建模型。模型（通常）是由层构成的图。最常见的模型类型是层的堆叠，keras.layers中就有很多模型，如下图：可以在源码文件中找到\n"),a("ul",[a("li",[s._v("tf.keras.Sequential模型(layers如下)")])])])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Dense\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" DepthwiseConv2D\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Dot\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Dropout\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" ELU\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Embedding\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Flatten\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" GRU\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" GRUCell\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" LSTMCell\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br")])]),a("ul",[a("li",[s._v("Flatten:将输入数据进行形状改变展开")]),s._v(" "),a("li",[s._v("Dense:添加一层神经元\n"),a("ul",[a("li",[s._v("Dense(units,activation=None,**kwargs)\n"),a("ul",[a("li",[s._v("units:神经元个数")]),s._v(" "),a("li",[s._v("activation：激活函数,参考tf.nn.relu,tf.nn.softmax,tf.nn.sigmoid,tf.nn.tanh")]),s._v(" "),a("li",[s._v("**kwargs:输入上层输入的形状，input_shape=()")])])])])])]),s._v(" "),a("p",[s._v("tf.keras.Sequential构建类似管道的模型")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("model = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation=tf.nn.relu),\n    keras.layers.Dense(10, activation=tf.nn.softmax)\n])\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("ul",[a("li",[a("strong",[s._v("Model类型：")])]),s._v(" "),a("li",[a("strong",[s._v("from")]),s._v(" keras.models "),a("strong",[s._v("import")]),s._v(" Model")]),s._v(" "),a("li",[s._v("Model(inputs=a, outputs=b)\n"),a("ul",[a("li",[s._v("inputs:定义模型的输入,Input类型")]),s._v(" "),a("li",[s._v("outpts:定义模型的输出")]),s._v(" "),a("li",[a("strong",[s._v("def")]),s._v(" "),a("strong",[s._v("call")]),s._v("(self, inputs):接收来自上层的输入")])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Model\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" Dense\n\ndata "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("784")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nout "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("input")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" output"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("out"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("h3",{attrs:{id:"models属性"}},[s._v("Models属性")]),s._v(" "),a("ul",[a("li",[a("code",[s._v("model.layers")]),s._v("：获取模型结构列表")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("core"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Flatten "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("object")]),s._v(" at "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0x10864a780")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("core"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Dense "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("object")]),s._v(" at "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0x10f95b128")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("core"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Dense "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("object")]),s._v(" at "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0x125bd6fd0")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("core"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Dense "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("object")]),s._v(" at "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0x125bf9240")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("ul",[a("li",[a("code",[s._v("model.inputs")]),s._v(" 是模型的输入张量列表")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Tensor "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'flatten_input:0'")]),s._v(" shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("?"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" dtype"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("float32"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("ul",[a("li",[a("code",[s._v("model.outputs")]),s._v(" 是模型的输出张量列表")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("outputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Tensor "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'dense_2/Softmax:0'")]),s._v(" shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("?"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" dtype"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("float32"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("ul",[a("li",[a("code",[s._v("model.summary()")]),s._v("打印模型的摘要表示")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("Layer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                50240     \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               8320      \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 59,850\nTrainable params: 59,850\nNon-trainable params: 0\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br")])]),a("h3",{attrs:{id:"models方法"}},[s._v("Models方法")]),s._v(" "),a("ul",[a("li",[a("p",[s._v("通过调用model的 "),a("code",[s._v("compile")]),s._v(" 方法去配置该模型所需要的训练参数以及评估方法。")])]),s._v(" "),a("li",[a("p",[s._v("model.compile(optimizer,loss=None,metrics=None, 准确率衡):配置训练相关参数")]),s._v(" "),a("ul",[a("li",[s._v("optimizer:梯度下降优化器(在keras.optimizers)")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Adadelta\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Adagrad\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Adam\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Adamax\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Nadam\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Optimizer\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" RMSprop\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" SGD\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" deserialize\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" get\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" serialize\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" AdamOptimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br")])]),a("ul",[a("li",[s._v("loss=None:损失类型,类型可以是字符串或者该function名字参考：")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" KLD\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" KLD "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" kld\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" KLD "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" kullback_leibler_divergence\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MAE\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MAE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" mae\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MAE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" mean_absolute_error\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MAPE\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MAPE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" mape\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MAPE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" mean_absolute_percentage_error\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MSE\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MSE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" mean_squared_error\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MSE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" mse\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MSLE\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MSLE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" mean_squared_logarithmic_error\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" MSLE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" msle\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" binary_crossentropy\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" categorical_crossentropy\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" categorical_hinge\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" cosine\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" cosine "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" cosine_proximity\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" deserialize\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" get\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" hinge\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" logcosh\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" poisson\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" serialize\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" sparse_categorical_crossentropy\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" squared_hinge\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br")])]),a("ul",[a("li",[s._v("metrics=None, ['accuracy']")])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Adam"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n              loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'sparse_categorical_crossentropy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n              metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("sparse_categorical_crossentropy:对于目标值是整型的进行交叉熵损失计算")]),s._v(" "),a("p",[s._v("categorical_crossentropy:对于两个output tensor and a target tensor进行交叉熵损失计算")]),s._v(" "),a("ul",[a("li",[a("p",[s._v("model.fit()：进行训练")]),s._v(" "),a("ul",[a("li",[a("p",[s._v("(x=None,y=None, batch_size=None,epochs=1,callbacks=None)")])]),s._v(" "),a("li",[a("p",[s._v("x:特征值:")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("、Numpy array "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("or")]),s._v(" array"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v("like"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("or")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("list")]),s._v(" of arrays\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("、A TensorFlow tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("or")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("list")]),s._v(" of tensors\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v("、`tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("data` dataset "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("or")]),s._v(" a dataset iterator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v(" Should "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("tuple")]),s._v(" of either `"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" targets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("` "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("or")]),s._v(" `"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" targets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" sample_weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("`"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v("、A generator "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("or")]),s._v(" `keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Sequence` returning `"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" targets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("` "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("or")]),s._v(" `"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" targets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" sample weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("`"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])])]),s._v(" "),a("li",[a("p",[s._v("y:目标值")])]),s._v(" "),a("li",[a("p",[s._v("batch_size=None：批次大小")])]),s._v(" "),a("li",[a("p",[s._v("epochs=1：训练迭代次数")])]),s._v(" "),a("li",[a("p",[a("strong",[s._v("callbacks=None：添加回调列表（用于如tensorboard显示等）")])])])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("train_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" train_labels"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("ul",[a("li",[s._v("model.evaluate(test_images, test_labels)")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("model.evaluate(test, test_label)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("ul",[a("li",[a("p",[s._v("预测model.predict(test)：")])]),s._v(" "),a("li",[a("p",[s._v("其它方法：")]),s._v(" "),a("ul",[a("li",[a("code",[s._v("model.save_weights(filepath)")]),s._v(" 将模型的权重保存为HDF5文件或者ckpt文件")]),s._v(" "),a("li",[a("code",[s._v("model.load_weights(filepath, by_name=False)")]),s._v("从HDF5文件（由其创建"),a("code",[s._v("save_weights")]),s._v("）加载模型的权重。默认情况下，架构预计不会更改。要将权重加载到不同的体系结构（具有一些共同的层），请使用"),a("code",[s._v("by_name=True")]),s._v("仅加载具有相同名称的那些层。")])])])]),s._v(" "),a("h2",{attrs:{id:"案例：实现多层神经网络进行时装分类"}},[s._v("案例：实现多层神经网络进行时装分类")]),s._v(" "),a("p",[s._v("70000 张灰度图像，涵盖 10 个类别。以下图像显示了单件服饰在较低分辨率（28x28 像素）下的效果：")]),s._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/%E6%9C%8D%E8%A3%85.png",alt:"服装"}})]),s._v(" "),a("h3",{attrs:{id:"需求："}},[s._v("需求：")]),s._v(" "),a("table",[a("thead",[a("tr",[a("th",[s._v("标签")]),s._v(" "),a("th",[s._v("类别")])])]),s._v(" "),a("tbody",[a("tr",[a("td",[s._v("0")]),s._v(" "),a("td",[s._v("T 恤衫/上衣")])]),s._v(" "),a("tr",[a("td",[s._v("1")]),s._v(" "),a("td",[s._v("裤子")])]),s._v(" "),a("tr",[a("td",[s._v("2")]),s._v(" "),a("td",[s._v("套衫")])]),s._v(" "),a("tr",[a("td",[s._v("3")]),s._v(" "),a("td",[s._v("裙子")])]),s._v(" "),a("tr",[a("td",[s._v("4")]),s._v(" "),a("td",[s._v("外套")])]),s._v(" "),a("tr",[a("td",[s._v("5")]),s._v(" "),a("td",[s._v("凉鞋")])]),s._v(" "),a("tr",[a("td",[s._v("6")]),s._v(" "),a("td",[s._v("衬衫")])]),s._v(" "),a("tr",[a("td",[s._v("7")]),s._v(" "),a("td",[s._v("运动鞋")])]),s._v(" "),a("tr",[a("td",[s._v("8")]),s._v(" "),a("td",[s._v("包包")])])])]),s._v(" "),a("h3",{attrs:{id:"步骤分析和代码实现："}},[s._v("步骤分析和代码实现：")]),s._v(" "),a("ul",[a("li",[s._v("读取数据集:\n"),a("ul",[a("li",[s._v("从datasets中获取相应的数据集，直接有训练集和测试集")])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("SingleNN")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("object")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fashion_mnist"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("load_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("ul",[a("li",[s._v("进行模型编写\n"),a("ul",[a("li",[s._v("双层：128个神经元，全连接层10个类别输出")]),s._v(" "),a("li",[s._v("Dense(128, activation=tf.nn.relu)：\n"),a("ul",[a("li",[s._v("定义网络结构以及初始化权重偏置参数")])])])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("SingleNN")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("object")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n    model "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n        keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("input_shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("softmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("p",[s._v("这里我们model只是放在类中，作为类的固定模型属性, 这里的激活函数暂时使用tf.nn.relu函数")]),s._v(" "),a("ul",[a("li",[s._v("编译定义优化过程:")])]),s._v(" "),a("p",[s._v("这里选择我们SGD优化器")]),s._v(" "),a("ul",[a("li",[s._v("keras.optimizers.SGD()")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("keras.optimizers.SGD(lr=0.01)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("loss:tf.keras.losses.sparse_categorical_crossentropy")]),s._v(" "),a("p",[s._v("metrics：accuracy")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n        SingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("SGD"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("lr"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.01")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                               loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sparse_categorical_crossentropy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                               metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("ul",[a("li",[s._v("训练:\n"),a("ul",[a("li",[a("strong",[s._v("关于迭代次数与每次迭代样本数")])]),s._v(" "),a("li",[s._v("设置batch_size=32或者128查看效果（值如何设置，优化部分会进行讲解）")])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("fit")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n        SingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("ul",[a("li",[s._v("评估:")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("evaluate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n        test_loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" test_acc "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" SingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("evaluate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("test_loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" test_acc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("p",[s._v("观察效果")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("Epoch 1/1\n   32/60000 [..............................] - ETA: 3:23 - loss: 14.7501 - acc: 0.0312\n  640/60000 [..............................] - ETA: 14s - loss: 14.2735 - acc: 0.1109 \n 1408/60000 [..............................] - ETA: 8s - loss: 13.4711 - acc: 0.1626 \n 2144/60000 [>.............................] - ETA: 7s - loss: 13.3273 - acc: 0.1721\n 2688/60000 [>.............................] - ETA: 6s - loss: 13.2748 - acc: 0.1752\n 2944/60000 [>.............................] - ETA: 7s - loss: 13.2538 - acc: 0.1766\n 3328/60000 [>.............................] - ETA: 7s - loss: 13.2791 - acc: 0.1752\n 3552/60000 [>.............................] - ETA: 7s - loss: 13.2540 - acc: 0.1768\n 4000/60000 [=>............................] - ETA: 7s - loss: 13.3192 - acc: 0.1727\n 4448/60000 [=>............................] - ETA: 7s - loss: 13.4163 - acc: 0.1668\n 5248/60000 [=>............................] - ETA: 6s - loss: 13.4322 - acc: 0.1658\n 5888/60000 [=>............................] - ETA: 6s - loss: 13.4066 - acc: 0.1675\n 6592/60000 [==>...........................] - ETA: 6s - loss: 13.3856 - acc: 0.1688\n 7040/60000 [==>...........................] - ETA: 5s - loss: 13.4107 - acc: 0.1673\n 7552/60000 [==>...........................] - ETA: 5s - loss: 13.4235 - acc: 0.1666\n 8064/60000 [===>..........................] - ETA: 5s - loss: 13.4387 - acc: 0.1657\n 8480/60000 [===>..........................] - ETA: 5s - loss: 13.4314 - acc: 0.1662\n 8928/60000 [===>..........................] - ETA: 5s - loss: 13.4435 - acc: 0.1654\n 9440/60000 [===>..........................] - ETA: 5s - loss: 13.4144 - acc: 0.1673\n10016/60000 [====>.........................] - ETA: 5s - loss: 13.3976 - acc: 0.1683\n10304/60000 [====>.........................] - ETA: 5s - loss: 13.3751 - acc: 0.1697\n10528/60000 [====>.........................] - ETA: 5s - loss: 13.3661 - acc: 0.1703\n10880/60000 [====>.........................] - ETA: 5s - loss: 13.3701 - acc: 0.1699\n11296/60000 [====>.........................] - ETA: 5s - loss: 13.3929 - acc: 0.1686\n12032/60000 [=====>........................] - ETA: 5s - loss: 13.4497 - acc: 0.1651\n12352/60000 [=====>........................] - ETA: 5s - loss: 13.4571 - acc: 0.1646\n12608/60000 [=====>........................] - ETA: 5s - loss: 13.4575 - acc: 0.1646\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br")])]),a("ul",[a("li",[a("strong",[s._v("效果不好，这里要进行特征归一化（优化部分会进行讲解）")])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 做特征值的归一化")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 所有像素值范围0~255,")]),s._v("\nself"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("/")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("255.0")]),s._v("\nself"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("/")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("255.0")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("Epoch 1/1\n   32/60000 [..............................] - ETA: 4:17 - loss: 2.4778 - acc: 0.1250\n  480/60000 [..............................] - ETA: 23s - loss: 2.2573 - acc: 0.1917 \n 1056/60000 [..............................] - ETA: 13s - loss: 2.0594 - acc: 0.3248\n 1632/60000 [..............................] - ETA: 10s - loss: 1.9308 - acc: 0.3909\n 2112/60000 [>.............................] - ETA: 9s - loss: 1.8408 - acc: 0.4304 \n 2464/60000 [>.............................] - ETA: 9s - loss: 1.7832 - acc: 0.4517\n 2816/60000 [>.............................] - ETA: 9s - loss: 1.7343 - acc: 0.4677\n 3296/60000 [>.............................] - ETA: 8s - loss: 1.6729 - acc: 0.4909\n 3712/60000 [>.............................] - ETA: 8s - loss: 1.6327 - acc: 0.5030\n 4032/60000 [=>............................] - ETA: 8s - loss: 1.5975 - acc: 0.5131\n 4480/60000 [=>............................] - ETA: 8s - loss: 1.5590 - acc: 0.5250\n 4768/60000 [=>............................] - ETA: 8s - loss: 1.5328 - acc: 0.5344\n 5248/60000 [=>............................] - ETA: 7s - loss: 1.4884 - acc: 0.5497\n 5664/60000 [=>............................] - ETA: 7s - loss: 1.4587 - acc: 0.5572\n 6304/60000 [==>...........................] - ETA: 7s - loss: 1.4136 - acc: 0.5692\n 7040/60000 [==>...........................] - ETA: 6s - loss: 1.3682 - acc: 0.5805\n 7392/60000 [==>...........................] - ETA: 6s - loss: 1.3480 - acc: 0.5875\n 7712/60000 [==>...........................] - ETA: 6s - loss: 1.3311 - acc: 0.5927\n 8128/60000 [===>..........................] - ETA: 6s - loss: 1.3119 - acc: 0.5979\n 8832/60000 [===>..........................] - ETA: 6s - loss: 1.2828 - acc: 0.6060\n 9568/60000 [===>..........................] - ETA: 6s - loss: 1.2503 - acc: 0.6156\n10080/60000 [====>.........................] - ETA: 6s - loss: 1.2283 - acc: 0.6216\n10528/60000 [====>.........................] - ETA: 5s - loss: 1.2136 - acc: 0.6260\n\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br")])]),a("h3",{attrs:{id:"打印模型"}},[s._v("打印模型")]),s._v(" "),a("ul",[a("li",[s._v("model.summary()：查看模型结构")])]),s._v(" "),a("h3",{attrs:{id:"手动保存和恢复模型"}},[s._v("手动保存和恢复模型")]),s._v(" "),a("ul",[a("li",[s._v("保存成ckpt形式\n"),a("ul",[a("li",[s._v("model.save_weights('./weights/my_model')")]),s._v(" "),a("li",[s._v("model.load_weights('./weights/my_model')")])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("SingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("save_weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./ckpt/SingleNN"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("predict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 直接使用训练过后的权重测试")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("exists"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./ckpt/checkpoint"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n    \tSingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("load_weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./ckpt/SingleNN"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    predictions "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" SingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 对预测结果进行处理")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("argmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br")])]),a("ul",[a("li",[s._v("保存成h5文件")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("SingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("save_weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./ckpt/SingleNN.h5"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("predict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 直接使用训练过后的权重测试")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("exists"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./ckpt/checkpoint"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n\n    \tSingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("load_weights"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"./ckpt/SingleNN.h5"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    predictions "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" SingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("argmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br")])]),a("h2",{attrs:{id:"fit的callbacks详解"}},[s._v("fit的callbacks详解")]),s._v(" "),a("p",[s._v("回调是在训练过程的给定阶段应用的一组函数。可以使用回调来获取培训期间内部状态和模型统计信息的视图。您可以将回调列表（作为关键字参数"),a("code",[s._v("callbacks")]),s._v("）传递给或类的"),a("code",[s._v("fit()")]),s._v("方法。然后将在训练的每个阶段调用回调的相关方法。")]),s._v(" "),a("ul",[a("li",[s._v("定制化保存模型")]),s._v(" "),a("li",[s._v("保存events文件")])]),s._v(" "),a("h3",{attrs:{id:"modelcheckpoint"}},[s._v("ModelCheckpoint")]),s._v(" "),a("p",[s._v("from tensorflow.python.keras.callbacks import ModelCheckpoint")]),s._v(" "),a("ul",[a("li",[s._v("keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=False, save_weights_only=False, mode='auto', period=1)\n"),a("ul",[a("li",[s._v("Save the model after every epoch：每隔多少次迭代保存模型")]),s._v(" "),a("li",[s._v("filepath: 保存模型字符串\n"),a("ul",[a("li",[s._v("如果设置 weights.{epoch:02d}-{val_loss:.2f}.hdf5格式，将会每隔epoch number数量并且将验证集的损失保存在该位置")]),s._v(" "),a("li",[s._v("如果设置weights.{epoch:02d}-{val_acc:.2f}.hdf5，将会按照val_acc的值进行保存模型")])])]),s._v(" "),a("li",[s._v("monitor: quantity to monitor.设置为'val_acc'或者'val_loss'")]),s._v(" "),a("li",[s._v("save_best_only: if save_best_only=True, 只保留比上次模型更好的结果")]),s._v(" "),a("li",[s._v("save_weights_only: if True, 只保存去那种(model.save_weights(filepath)), else the full model is saved (model.save(filepath)).")]),s._v(" "),a("li",[s._v("mode: one of {auto, min, max}. 如果save_best_only=True, 对于val_acc, 要设置max, 对于val_loss要设置min")]),s._v(" "),a("li",[s._v("period: 迭代保存checkpoints的间隔")])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("check "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" ModelCheckpoint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'./ckpt/singlenn_{epoch:02d}-{val_acc:.2f}.h5'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                                monitor"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'val_acc'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                                save_best_only"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                                save_weights_only"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                                mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'auto'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                                period"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nSingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" callbacks"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br")])]),a("h3",{attrs:{id:"tensorboard"}},[s._v("Tensorboard")]),s._v(" "),a("ul",[a("li",[s._v("添加Tensorboard观察损失等情况")]),s._v(" "),a("li",[s._v("keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n"),a("ul",[a("li",[s._v("log_dir:保存事件文件目录")]),s._v(" "),a("li",[s._v("write_graph=True：是否显示图结构")]),s._v(" "),a("li",[s._v("write_images=False：是否显示图片")]),s._v(" "),a("li",[s._v("write_grads=True:是否显示梯度"),a("code",[s._v("histogram_freq")]),s._v(" 必须大于0")])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 添加tensoboard观察")]),s._v("\n tensorboard "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("callbacks"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("TensorBoard"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("log_dir"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'./graph'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" histogram_freq"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                                                  write_graph"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" write_images"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\nSingleNN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" callbacks"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("tensorboard"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])])])},[],!1,null,null,null);t.default=e.exports}}]);