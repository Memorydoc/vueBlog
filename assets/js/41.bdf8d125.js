(window.webpackJsonp=window.webpackJsonp||[]).push([[41],{314:function(t,s,a){"use strict";a.r(s);var r=a(10),n=Object(r.a)({},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"神经网络原理"}},[t._v("神经网络原理")]),t._v(" "),a("h2",{attrs:{id:"学习目标"}},[t._v("学习目标")]),t._v(" "),a("ul",[a("li",[t._v("目标\n"),a("ul",[a("li",[t._v("说明神经网络的分类原理")]),t._v(" "),a("li",[t._v("说明softmax回归")]),t._v(" "),a("li",[t._v("说明交叉熵损失")])])]),t._v(" "),a("li",[t._v("应用\n"),a("ul",[a("li",[t._v("无")])])])]),t._v(" "),a("p",[t._v("神经网络的主要用途在于分类，那么整个神经网络分类的原理是怎么样的？我们还是围绕着损失、优化这两块去说。神经网络输出结果如何分类？")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A6%82%E4%BD%95%E5%88%86%E7%B1%BB.png",alt:"神经网络如何分类"}})]),t._v(" "),a("p",[a("strong",[t._v("神经网络解决多分类问题最常用的方法是设置n个输出节点，其中n为类别的个数。")])]),t._v(" "),a("p",[t._v("任意事件发生的概率都在0和1之间，且总有某一个事件发生（概率的和为1）。如果将分类问题中“一个样例属于某一个类别”看成一个概率事件，那么训练数据的正确答案就符合一个概率分布。如何将神经网络前向传播得到的结果也变成概率分布呢？Softmax回归就是一个常用的方法。")]),t._v(" "),a("h2",{attrs:{id:"softmax回归"}},[t._v("softmax回归")]),t._v(" "),a("p",[t._v("Softmax回归将神经网络输出转换成概率结果")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/softmax%E5%85%AC%E5%BC%8F.png",alt:"softmax公式"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/softmax%E5%9B%9E%E5%BD%92.png",alt:"softmax回归"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/softmax%E5%B1%95%E5%BC%80.png",alt:"softmax展开"}})]),t._v(" "),a("ul",[a("li",[t._v("softmax特点")])]),t._v(" "),a("p",[t._v("如何理解这个公式的作用呢？看一下计算案例")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("假设输出结果为："),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.6")]),t._v("\nsoftmax的计算输出结果为：\ny1_p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.3")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.3")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ny1_p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.3")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ny1_p "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.6")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.3")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.1")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("p",[a("strong",[t._v("这样就把神经网络的输出也变成了一个概率输出")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/%E8%AF%86%E5%88%AB%E8%BE%93%E5%87%BA%E6%A6%82%E7%8E%87.png",alt:"识别输出概率"}})]),t._v(" "),a("h4",{attrs:{id:"那么如何去衡量神经网络预测的概率分布和真实答案的概率分布之间的距离？"}},[t._v("那么如何去衡量神经网络预测的概率分布和真实答案的概率分布之间的距离？")]),t._v(" "),a("h2",{attrs:{id:"交叉熵损失"}},[t._v("交叉熵损失")]),t._v(" "),a("h3",{attrs:{id:"公式"}},[t._v("公式")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/%E4%BA%A4%E5%8F%89%E6%8D%9F%E5%A4%B1%E7%90%86%E8%A7%A3.png",alt:"交叉熵损失公式"}})]),t._v(" "),a("p",[t._v("为了能够衡量距离，目标值需要进行one-hot编码，能与概率值一一对应，如下图")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/%E4%BA%A4%E5%8F%89%E6%8D%9F%E5%A4%B1%E7%90%86%E8%A7%A3.png",alt:"交叉损失理解"}})]),t._v(" "),a("p",[t._v("它的损失如何计算？")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("0log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("0log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("0log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.15")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("0log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("0log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("0log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("1log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("0log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.05")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("0log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("0log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("**上述的结果为1log(0.10)，那么为了减少这一个样本的损失。神经网络应该怎么做？**所以会提高对应目标值为1的位置输出概率大小，由于softmax公式影响，其它的概率必定会减少。只要这样进行调整这样是不是就预测成功了！！！！！")]),t._v(" "),a("h3",{attrs:{id:"提高对应目标值为1的位置输出概率大小"}},[t._v("提高对应目标值为1的位置输出概率大小")]),t._v(" "),a("h3",{attrs:{id:"损失大小"}},[t._v("损失大小")]),t._v(" "),a("p",[t._v("神经网络最后的损失为平均每个样本的损失大小。对所有样本的损失求和取其平均值")]),t._v(" "),a("h2",{attrs:{id:"梯度下降算法"}},[t._v("梯度下降算法")]),t._v(" "),a("p",[t._v("目的：使损失函数的值找到最小值")]),t._v(" "),a("p",[t._v("方式：梯度下降")]),t._v(" "),a("p",[t._v("函数的**梯度（gradient）**指出了函数的最陡增长方向。"),a("strong",[t._v("梯度的方向走，函数增长得就越快。那么按梯度的负方向走，函数值自然就降低得最快了")]),t._v("。模型的训练目标即是寻找合适的 w 与 b 以最小化代价函数值。假设 "),a("strong",[t._v("w 与 b 都是一维实数")]),t._v("，那么可以得到如下的 J 关于 w 与 b 的图：")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%9B%BE.png",alt:""}})]),t._v(" "),a("p",[t._v("可以看到，此成本函数 J 是一个"),a("strong",[t._v("凸函数")])]),t._v(" "),a("p",[t._v("参数w和b的更新公式为：")]),t._v(" "),a("p",[t._v("$$w := w - \\alpha\\frac{dJ(w, b)}{dw}$$，$$b := b - \\alpha\\frac{dJ(w, b)}{db}$$")]),t._v(" "),a("blockquote",[a("p",[t._v("注：其中 α 表示学习速率，即每次更新的 w 的步伐长度。当 w 大于最优解 w′ 时，导数大于 0，那么 w 就会向更小的方向更新。反之当 w 小于最优解 w′ 时，导数小于 0，那么 w 就会向更大的方向更新。迭代直到收敛。")])]),t._v(" "),a("p",[t._v("通过平面来理解梯度下降过程：")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%90%86%E8%A7%A3.png",alt:""}})]),t._v(" "),a("h2",{attrs:{id:"网络原理总结"}},[t._v("网络原理总结")]),t._v(" "),a("p",[t._v("我们不会详细地讨论可以如何使用反向传播和梯度下降等算法训练参数。"),a("strong",[t._v("训练过程中的计算机会尝试一点点增大或减小每个参数，看其能如何减少相比于训练数据集的误差，以望能找到最优的权重、偏置参数组合")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%BC%94%E7%A4%BA.png",alt:"传播算法演示"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/articial/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E5%8A%A8%E6%80%81%E5%9B%BE.gif",alt:"网络优化动态图"}})])])},[],!1,null,null,null);s.default=n.exports}}]);